const fs = require("fs-extra");

const { Octokit } = require("@octokit/core");
const { paginateRest } = require("@octokit/plugin-paginate-rest");
const hash = require("object-hash");
const shell = require("shelljs");
const path = require("path");

const downloadTarball = require("download-tarball");

const makeMdxFilesFromSphinxJson = require("./splitgraph-docs/makeMdxFilesFromSphinxJson.js");

if (!shell.which("git")) {
  console.error("Error: git not installed");
  shell.exit(1);
}

const secrets = {
  githubAccessToken: process.env.SPLITBOT_GITHUB_ACCESS_TOKEN,
};

const getConfig = ({ tmpDir }) => ({
  auth: secrets.githubAccessToken,
  upstream: {
    owner: "splitgraph",
    repo: "splitgraph",
  },
  getCommitMessage: () => {
    return "Testing push";
  },
  cloneTo: path.join(tmpDir, "splitgraph.com"),
  author: {
    name: "Splitbot",
    email: "splitbot@splitgraph.dev",
  },
  docs: {
    // Update the docs at this repo
    owner: "splitgraph",
    repo: "splitgraph.com",
    gitUrl: `https://splitbot:${secrets.githubAccessToken}@github.com/splitgraph/splitgraph.com`,

    // Maintain a constant branch name and push updates to it
    // This means there will only ever be one PR open at a time, which is ideal,
    // but it does have the small downside that human edits may be force pushed
    // over. This could be mitigated, by checking in the beginning if a PR is
    // already open. If it is, the program could base on the head of the
    // existing PR instead of the base of the existing PR.
    newBranch: `splitbot/update-docs`,

    // Submit a pull request against this branch
    targetBranch: "master",

    // When creating/modifying a version, call these functions according to
    // the name of each subdirectory in the extracted tarball of the asset
    processors: {
      asciinema: processAsciinema,
      json: processPythonApiDocs,
      sgr: processSgrCliDocs,
      "0100_config-flag-reference.mdx": processConfigFlagReference,
    },

    // When deleting a version, call each one of these functions
    deleters: [
      deleteAsciinemaVersion,
      deletePythonApiDocsVersion,
      deleteSgrCliDocsVersion,
    ],

    // Optionally give a title to the resources in the bundle
    // Key names must match keys in returned in paths() callback (see below)
    titles: {
      sgrCliDocs: "sgr CLI",
      pythonApiDocs: "Python API",
    },

    // Use these paths in the repo to find autogenerated docs/manifest
    paths: ({ cloneTo }) => ({
      manifest: path.join(cloneTo, "content/docs-manifest.json"),
      configFlagReference: path.join(
        cloneTo,
        "content/docs/1000_configuration/0100_config-flag-reference.mdx"
      ),
      asciinema: {
        latest: path.join(cloneTo, "content/casts/versioned/latest"),
        archive: path.join(cloneTo, "content/casts/versioned/archive"),
      },
      sgrCliDocs: {
        latest: path.join(cloneTo, "content/docs/9000_sgr"),
        archive: path.join(cloneTo, "content/docs/9000_sgr/9000_versions"),
      },
      pythonApiDocs: {
        latest: path.join(cloneTo, "content/docs/9500_python-api"),
        archive: path.join(
          cloneTo,
          "content/docs/9500_python-api/9000_versions"
        ),
      },
    }),
  },
  assetIsDocs: (asset) => {
    const { name } = asset;

    return name.includes("docs");
  },
});

const main = async () => {
  const tmpDir = mkTempDir();
  const config = getConfig({ tmpDir });

  await cloneRepo({ config });

  const client = makeClient({ config });

  const manifest = await getCurrentManifest({ config });
  const releases = await fetchReleases({ client, config });

  const diff = await compareReleases({
    manifest,
    releases,
    client,
    config,
  });

  const commitMessage = await makeCommitMessage(diff);

  const { anyChanged, latest, unchanged, created, deleted, modified } = diff;

  console.log(
    JSON.stringify(
      { anyChanged, latest, unchanged, created, deleted, modified },
      null,
      2
    )
  );

  if (!anyChanged) {
    console.error("No changes");
    return;
  }

  await applyManifest({
    config,
    manifest,
    latest,
    unchanged,
    created,
    deleted,
    modified,
  });

  // A creation is a release that exists in the upstream list of releases but not in the manifest
  // Apply it by extracting the upstream release to the appropriate local directory
  await applyCreated({ config, created });

  // A modification is when the manifest and remote both have a release with
  // the same TAG, but at least one other aspect of the release is different
  // Apply it by overwriting the local files with the contents of the remote
  // Note: this should hopefully be rare, or else not happen once a human has
  //       edited the local files
  await applyModified({ config, modified });

  // A deletion is an item that we have in the local manifest but not in the remote
  // Apply it by deleting it locally
  await applyDeleted({ config, deleted });

  // Since we save all versions independent of the copy in "latest" (you
  // can always access the latest version at its version number too), we
  // simply overwrite the "latest" contents every time
  await overwriteLatest({ config, newLatest: latest });

  await makeCommit({ config, commitMessage });

  await pushToRemote({ config });

  await makePullRequestIfNecessary({ config, client, commitMessage });
};

const deleteLatestDocs = async ({ latestDir, archiveDir }) => {
  if (!latestDir || !archiveDir) {
    throw new Error("Missing latestDir or archiveDir path for python API");
  }

  // "latestDir" is at the top level of the directory, and not in a version
  // subdirectory. therefore we want to delete everything that is not one
  // of two protected files: (1) metadata.json, and (2) the versions folder
  // (note in the case of asciinema, the latest dir *is* its own directory,
  //  but we still want to protect metadata.json regardless)

  await fs.mkdirp(latestDir);
  await fs.ensureFile(path.join(latestDir, ".gitkeep"));
  await fs.mkdirp(archiveDir);
  await fs.ensureFile(path.join(archiveDir, ".gitkeep"));

  const cleanupCommand = `find ${latestDir} -maxdepth 1 \
                            ! -name metadata.json \
                            ! -name ${path.basename(archiveDir)} \
                            ! -name .gitkeep \
                            ! -path ${latestDir} \
                            -print \
                            -exec rm -rf {} \\;`; // note double escape necessary

  const cleanupResult = shell.exec(cleanupCommand);

  if (cleanupResult.code !== 0) {
    throw new Error(
      `Error cleaning latest at ${latestDir}: ${cleanupResult.stderr}`
    );
  }

  return;
};

const deleteVersionOfDocs = async ({ archiveDir, tag_name }) => {
  if (!archiveDir || !tag_name) {
    throw new Error("Missing archiveDir or tag_name in deleteVersionOfDocs");
  }

  const versionDir = path.join(archiveDir, tag_name);

  const deleteVersionCmd = `rm -rf ${versionDir}`;
  const deleteVersionResult = shell.exec(deleteVersionCmd);

  // This could be ok (maybe it exists in manifest but no dir exists)
  if (deleteVersionResult.code !== 0) {
    console.error("Warning: Failed to rm version dir:", versionDir);
  }
};

const findDocsAsset = ({ config, assets = [] }) => {
  return assets.find(config.assetIsDocs);
};

const normalizeAsset = (asset) => {
  const {
    id,
    url,
    name,
    label,
    created_at,
    updated_at,
    browser_download_url,
  } = asset;

  return { id, url, name, label, created_at, updated_at, browser_download_url };
};

const normalizeAssets = ({ config, assets }) => {
  return assets.filter(config.assetIsDocs).map(normalizeAsset);
};

const normalizeRelease = ({ config, release }) => {
  const {
    id,
    tag_name,
    target_commitish,
    created_at,
    published_at,
    assets,
  } = release;
  return {
    id,
    tag_name,
    target_commitish,
    created_at,
    published_at,
    assets: normalizeAssets({ config, assets }),
  };
};

const releaseSorterByDateAscending = (r1, r2) =>
  new Date(r2.published_at) - new Date(r1.published_at);

// Remove uannecssary fields, sort by recent first, remove duplicates (!)
const normalizeReleases = ({ config, releases }) => {
  // Sort most recent first
  const sortedReleases = releases.sort(releaseSorterByDateAscending);

  // Pick the first of each tag_name
  const dedupedReleases = [];
  while (sortedReleases.length > 0) {
    const nextRelease = sortedReleases.shift();

    if (!findMatchingTag(dedupedReleases, nextRelease)) {
      dedupedReleases.push(nextRelease);
    }
  }

  const normalizedReleases = dedupedReleases
    .map((release) => normalizeRelease({ release, config }))
    .filter((release) => release.assets.length > 0);

  return normalizedReleases;
};

const makeClient = ({ config }) => {
  const PaginatedOctokit = Octokit.plugin(paginateRest);
  const client = new PaginatedOctokit({ auth: config.auth });

  return client;
};

const fetchReleases = async ({ client, config }) => {
  const releases = await client.paginate(
    "GET /repos/:owner/:repo/releases",
    {
      owner: config.upstream.owner,
      repo: config.upstream.repo,
      per_page: 1,
    },
    (response, done) => {
      return normalizeReleases({ config, releases: response.data });
    }
  );

  return releases;
};

/*
  returns e.g.

  {
    anyChanged: true,
    latest: { ...release },
    unchanged: [],
    created: [],
    deleted: [],
    modified: []
  }
*/
const compareReleases = async ({ releases, manifest, client, config }) => {
  const manifestReleases = manifest.releases;

  const unchanged = findUnchanged({
    manifestReleases,
    remoteReleases: releases,
  });
  const created = findCreated({ manifestReleases, remoteReleases: releases });
  const modified = findModified({ manifestReleases, remoteReleases: releases });
  const deleted = findDeleted({ manifestReleases, remoteReleases: releases });
  const latest = await findLatest({ client, remoteReleases: releases, config });
  const latestChanged = !releasesAreEqual(manifest.latest, latest);
  const anyChanged =
    created.length > 0 ||
    modified.length > 0 ||
    deleted.length > 0 ||
    latestChanged;

  return {
    latestChanged,
    anyChanged,
    unchanged,
    latest,
    created,
    modified,
    deleted,
  };
};

const findLatest = async ({ client, remoteReleases, config }) => {
  const { data } = await client.request(
    "GET /repos/:owner/:repo/releases/latest",
    {
      owner: config.upstream.owner,
      repo: config.upstream.repo,
    }
  );

  var latest = remoteReleases.find((release) => release.id === data.id);

  // In the case a release is published to github which does not include any
  // docs assets, it would be filtered in normalizeReleases, so latest would be undefined
  // Just take the safe option and use the most recent, in this case
  if (!latest && remoteReleases.length > 0) {
    latest = remoteReleases[0];
  }

  return latest || {};
};

const findMatchingTag = (list, release) =>
  list.find((x) => x.tag_name === release.tag_name);

const findConflictingTag = (list, release) =>
  list.find((x) => x.tag_name === release.tag_name && !releasesAreEqual);

const findMatchingRelease = (list, release) =>
  list.find((x) => releasesAreEqual(x, release));

const findUnchanged = ({ manifestReleases, remoteReleases }) => {
  return manifestReleases.filter(
    (release) => !!findMatchingRelease(remoteReleases, release)
  );
};

// release exists on remote but not manifest
// but, we don't want to double count modified
const findCreated = ({ manifestReleases, remoteReleases }) => {
  const created = remoteReleases.filter(
    (release) =>
      !findMatchingRelease(manifestReleases, release) &&
      !findConflictingTag(manifestReleases, release)
  );

  return created;
};

// tag is same, but fingerprint of release is different
const findModified = ({ manifestReleases, remoteReleases }) => {
  return remoteReleases.filter(
    (release) => !!findConflictingTag(manifestReleases, release)
  );
};

// tag exists in manifest but not in remote
const findDeleted = ({ manifestReleases, remoteReleases }) => {
  return manifestReleases.filter(
    (release) => !findMatchingTag(remoteReleases, release)
  );
};

const fingerprintAsset = (asset) => hash(asset);
const fingerprintAssets = (assets) =>
  hash(new Set((assets || []).map(fingerprintAsset)));

const fingerprintRelease = (release) => {
  const { id, tag_name, target_commitish, created_at, published_at } = release;

  const assetHash = fingerprintAssets(release.assets);

  return hash({
    id,
    tag_name,
    target_commitish,
    created_at,
    published_at,
    assetHash,
  });
};

const releasesAreEqual = (releaseA, releaseB) => {
  const equal = fingerprintRelease(releaseA) === fingerprintRelease(releaseB);
  return equal;
};

const getEmptyManifest = () => {
  return {
    // a tag which matches at least one release (prefer the most recent)
    latest: {},

    // releases, most recent first
    releases: [],
  };
};

const getCurrentManifest = async ({ config }) => {
  try {
    return require(config.docs.paths(config).manifest);
  } catch {
    console.error("Warning: using empty manifest");
    return getEmptyManifest();
  }
};

const applyManifest = async ({
  config,
  manifest,
  latest,
  unchanged,
  created,
  deleted,
  modified,
}) => {
  const nextReleases = [...unchanged, ...created]
    .filter((release) => !findMatchingRelease(deleted, release))
    .map((release) =>
      findConflictingTag(modified, release)
        ? findMatchingTag(modified, release)
        : release
    )
    .sort(releaseSorterByDateAscending);

  const nextManifest = {
    latest,
    releases: nextReleases,
  };

  await fs.writeJson(config.docs.paths(config).manifest, nextManifest, {
    spaces: 2,
  });

  return nextManifest;
};

const mkTempDir = () => {
  const { stdout, stderr, code } = shell.exec("mktemp -d", { silent: true });

  if (code !== 0) {
    throw new Error({ message: `Error in mkTempDir: ${stderr}` });
  }

  const tempDir = stdout.trim();
  return tempDir;
};

const cloneRepo = async ({ config }) => {
  const { cloneTo, author } = config;

  const { targetBranch, gitUrl, newBranch } = config.docs;

  if (!cloneTo || !targetBranch || !gitUrl || !newBranch) {
    throw new Error("Missing one of: cloneTo, targetBranch, gitUrl, newBranch");
  }

  console.error("Clone to:", cloneTo);

  const cloneCmd = `git clone --depth 1 -b ${targetBranch} ${gitUrl} ${cloneTo}`;
  const cloneResult = shell.exec(cloneCmd);
  if (cloneResult.code !== 0) {
    throw new Error({ message: `Error in clone: ${cloneResult.stderr}` });
  }

  shell.pushd(cloneTo);

  const checkoutCmd = `git checkout -b ${newBranch}`;
  const checkoutResult = shell.exec(checkoutCmd);

  if (checkoutResult.code !== 0) {
    throw new Error({ message: `Error in checkout: ${checkoutResult.stderr}` });
  }

  const setGitNameCmd = `git config user.name "${author.name}"`;
  const setGitNameResult = shell.exec(setGitNameCmd);
  if (setGitNameResult.code !== 0) {
    throw new Error({
      message: `Error in git config: ${setGitNameResult.stderr}`,
    });
  }

  const setGitEmailCmd = `git config user.email "${author.email}"`;
  const setGitEmailResult = shell.exec(setGitEmailCmd);
  if (setGitEmailResult.code !== 0) {
    throw new Error({
      message: `Error in git config: ${setGitEmailResult.stderr}`,
    });
  }

  shell.popd();

  return;
};

const addMetadata = async ({
  release,
  metadataDir,
  title,
  isLatest = false,
  keepTitleIfExists = false,
}) => {
  const { tag_name } = release;

  const metadataFile = path.join(metadataDir, "metadata.json");
  const metadataFileExists = await fs.pathExists(metadataFile);

  const existingMetadata = metadataFileExists ? require(metadataFile) : {};

  // If existing metadata has title that is anything other than the tag_name
  // of the release, assume it was edited by a human and leave it.
  const metadata = {
    title:
      keepTitleIfExists &&
      existingMetadata &&
      existingMetadata.title &&
      existingMetadata.title !== tag_name
        ? existingMetadata.title
        : title || tag_name,
    updated_at: new Date(),
    release,
    isLatest,
  };

  await fs.writeJson(metadataFile, metadata, {
    spaces: 2,
  });
};

const getOrMakeOutputDirForRelease = async ({
  release,
  archiveDir,
  overwrite = true,
}) => {
  const { tag_name } = release;
  const outputDir = path.join(archiveDir, tag_name);
  const outputDirExists = await fs.pathExists(outputDir);

  if (outputDirExists && overwrite) {
    await fs.remove(outputDir);
  }

  await fs.mkdirp(outputDir);

  return outputDir;
};

const genericCopyAssetResource = async ({ archiveDir, release, inputDir }) => {
  const outputDir = await getOrMakeOutputDirForRelease({ release, archiveDir });

  await fs.copy(inputDir, outputDir);

  await addMetadata({ release, metadataDir: outputDir });
};

// To process asciinema, simply copy the files from bundle to their destination
const processAsciinema = async ({ config, release, inputDir }) => {
  const { archive } = config.docs.paths(config).asciinema;
  await genericCopyAssetResource({
    archiveDir: archive,
    release,
    inputDir,
  });
  return;
};

// To process the config file reference mdx, copy it to the right folder
// Note that inputDir here is actually the file, not the dir
const processConfigFlagReference = async ({ config, release, inputDir }) => {
  const inputMdx = inputDir;
  const outputMdx = config.docs.paths(config).configFlagReference;

  await fs.copy(inputMdx, outputMdx);

  return;
};

// To process sgr-cli, simply copy the files from bundle to their destination
const processSgrCliDocs = async ({ config, release, inputDir }) => {
  const { archive } = config.docs.paths(config).sgrCliDocs;
  await genericCopyAssetResource({
    archiveDir: archive,
    release,
    inputDir,
  });
  return;
};

// To process sgr-cli, call out to makeMdxFilesFromSphinxJson
// Note the inputDir (top level) is "json", but we are interested in json/api,
// which is the direct parent of the .fjson files
const processPythonApiDocs = async ({ config, release, inputDir }) => {
  const { archive } = config.docs.paths(config).pythonApiDocs;

  const outputDir = await getOrMakeOutputDirForRelease({
    release,
    archiveDir: archive,
  });

  console.warn("inputDir:", path.join(inputDir, "api"));
  console.warn("outputDir:", outputDir);
  // process.exit(0);

  makeMdxFilesFromSphinxJson({
    inputDir: path.join(inputDir, "api"),
    outputDir,
  });

  await addMetadata({ release, metadataDir: outputDir });
};

const processAsset = async ({ config, release, asset }) => {
  const { browser_download_url } = asset;

  const downloadLocation = mkTempDir();

  await downloadTarball({
    url: browser_download_url,
    dir: downloadLocation,
  });

  const tarballName = path
    .basename(browser_download_url)
    .replace(/\.tar(\.gz)?/, "");
  const extractedDir = path.join(downloadLocation, tarballName);

  const extractedDirExists = await fs.pathExists(extractedDir);

  if (!extractedDirExists) {
    throw new Error(
      "Expected extraction directory does not exist at:",
      extractedDir
    );
  }

  console.error("Extract tarball to:", extractedDir);

  const assetResources = shell.ls(extractedDir).map((resource) => ({
    resourceName: resource,
    resourceDir: path.join(extractedDir, resource),
  }));

  for (let { resourceName, resourceDir } of assetResources) {
    const processAssetResource = config.docs.processors[resourceName];

    console.error(
      `apply processors[${resourceName}]`,
      config.docs.processors[resourceName]
    );

    await processAssetResource({
      config,
      asset,
      release,
      inputDir: resourceDir,
    });
  }
};

const applyCreated = async ({ config, created }) => {
  for (let release of created) {
    for (let asset of release.assets) {
      await processAsset({ config, release, asset });
    }
  }
};

const applyModified = async ({ config, modified }) => {
  for (let release of modified) {
    for (let asset of release.assets) {
      await processAsset({ config, release, asset });
    }
  }
};

const deleteAsciinemaVersion = async ({ config, tag_name }) =>
  await deleteVersionOfDocs({
    archiveDir: config.docs.paths(config).asciinema.archive,
    tag_name,
  });

const deletePythonApiDocsVersion = async ({ config, tag_name }) =>
  await deleteVersionOfDocs({
    archiveDir: config.docs.paths(config).pythonApiDocs.archive,
    tag_name,
  });

const deleteSgrCliDocsVersion = async ({ config, tag_name }) =>
  await deleteVersionOfDocs({
    archiveDir: config.docs.paths(config).sgrCliDocs.archive,
    tag_name,
  });

const applyDeleted = async ({ config, deleted }) => {
  const deletedTags = (deleted || []).map(({ tag_name }) => tag_name);

  for (let deletedTag of deletedTags) {
    for (let deleter of config.docs.deleters) {
      console.error("apply deleter", deleter);
      await deleter({
        config,
        tag_name: deletedTag,
      });
    }
  }
};

const copyLatestDocsFromArchive = async ({
  latestDir,
  archiveDir,
  newLatest,
}) => {
  const newVersion = newLatest.tag_name;
  const newVersionDir = path.join(archiveDir, newVersion);

  const newVersionExists = await fs.pathExists(newVersionDir);

  // This used to throw an error instead of a warning. It will happen when a
  // resource root did not have any updates in the latest version (e.g., the
  // `casts` directory stopped existing after `v3.0.0`).
  if (!newVersionExists) {
    console.warn(
      `WARNING: cannot copy ${newVersion} at ${newVersionDir} (does not exist) to ${latestDir}`
    );
    return;
  }

  await fs.copy(newVersionDir, latestDir);
};

const overwriteLatest = async ({ config, newLatest }) => {
  const pathMaps = config.docs.paths(config);
  const titleMap = config.docs.titles;

  for (let resourceRoot of Object.keys(pathMaps)) {
    if (!pathMaps[resourceRoot].latest) {
      continue;
    }

    let title = titleMap[resourceRoot];
    let latestDir = pathMaps[resourceRoot].latest;
    let archiveDir = pathMaps[resourceRoot].archive;

    await deleteLatestDocs({
      config,
      latestDir,
      archiveDir,
    });

    await copyLatestDocsFromArchive({ latestDir, archiveDir, newLatest });

    // Note asciinema have no metadata
    if (title) {
      await addMetadata({
        release: newLatest,
        isLatest: true,
        keepTitleIfExists: true,
        metadataDir: latestDir,
        title,
      });
    }
  }
};

const sleep = (seconds) =>
  new Promise((resolve) => setTimeout(resolve, seconds * 1000));

// Given the (new) manifest, make a commit message describing it
// Note any quotes would need to be escaped (so avoid them)
const makeCommitMessage = async ({
  latest,
  unchanged,
  created,
  deleted,
  modified,
}) => {
  var message = "";
  message += `Update docs (latest: ${latest.tag_name})`;

  message += `\n- Versions created: ${
    !created || created.length === 0 ? "None" : created.length
  }`;
  for (let { tag_name } of unchanged) {
    message += `\n    - ${tag_name}`;
  }

  message += `\n- Versions modified: ${
    !modified || modified.length === 0 ? "None" : modified.length
  }`;
  for (let { tag_name } of modified) {
    message += `\n    - ${tag_name}`;
  }

  message += `\n- Versions deleted: ${
    !deleted || deleted.length === 0 ? "None" : deleted.length
  }`;
  for (let { tag_name } of deleted) {
    message += `\n    - ${tag_name}`;
  }

  // Don't list unchanged because it is presumably a growing list
  message += `\n- Versions unchanged: ${
    !unchanged || unchanged.length === 0 ? "None" : unchanged.length
  }`;

  return message;
};

const makeCommit = async ({ config, commitMessage }) => {
  console.log("Making commit:");
  console.log(commitMessage);

  const { cloneTo } = config;
  shell.pushd(cloneTo);

  const stageDeletedFilesCmd = `test $(git ls-files -d | wc -l) -gt 0 && git ls-files -d | xargs git rm || true`;
  const stageDeletedFilesResult = shell.exec(stageDeletedFilesCmd);
  if (stageDeletedFilesResult.code !== 0) {
    throw new Error("Error staging deleted files:");
  }

  // seems to be necessary for git
  await sleep(1);

  const stageUntrackedFilesCmd = `test $(git ls-files -o | wc -l) -gt 0 && git ls-files -o | xargs git add || true`;
  const stageUntrackedFilesResult = shell.exec(stageUntrackedFilesCmd);
  if (stageUntrackedFilesResult.code !== 0) {
    throw new Error("Error staging untracked files:");
  }

  // seems to be necessary for git
  await sleep(1);

  const stageModifiedFilesCmd = `test $(git ls-files -m | wc -l) -gt 0 && git ls-files -m | xargs git add || true`;
  const stageModifiedFilesResult = shell.exec(stageModifiedFilesCmd);
  if (stageModifiedFilesResult.code !== 0) {
    throw new Error("Error staging modified files:");
  }

  // seems to be necessary for git
  await sleep(1);

  const commitCmd = `git commit -m "${commitMessage}"`;
  const commitResult = shell.exec(commitCmd);
  if (commitResult.code !== 0) {
    throw new Error("Error during commit:");
  }

  shell.popd();

  return;
};

const pushToRemote = async ({ config }) => {
  const { cloneTo } = config;

  const { newBranch } = config.docs;

  const gitPushCmd = `git push --force origin ${newBranch}:${newBranch}`;

  shell.pushd(cloneTo);
  const { stderr, code } = shell.exec(gitPushCmd);
  shell.popd();

  if (code !== 0) {
    throw new Error(`Error pushing to remote: ${stderr}`);
  }

  return;
};

const makePullRequestIfNecessary = async ({
  config,
  client,
  commitMessage,
}) => {
  // Check if PR already exists for this branch and base
  const { data } = await client.request("GET /repos/:owner/:repo/pulls", {
    owner: config.docs.owner,
    repo: config.docs.repo,
    head: `${config.docs.owner}:${config.docs.newBranch}`,
    base: config.docs.targetBranch,
    state: "open",
  });

  const alreadyExists = data.length > 0;

  if (alreadyExists) {
    console.error("Pull request already open for branch, do not open again");
    return;
  }

  await client.request("POST /repos/:owner/:repo/pulls", {
    owner: config.docs.owner,
    repo: config.docs.repo,
    title: commitMessage.split("\n")[0],
    head: config.docs.newBranch,
    base: config.docs.targetBranch,
    body: commitMessage,
  });

  return;
};

const yargs = require("yargs").command(
  "$0",
  `
Download docs from github releases and sync with @splitgraph/content by
overwriting files in directory when necessary.
Then submit a PR to splitgraph/splitgraph.com

- Fetch all releases at github:splitgraph/splitgraph that include docs asset

- For each asset, compare {  } to entry in local docs/manifest.json

- Check the remote /latest release against local /latst release

  `,
  (yargs) => {
    yargs;
  }
);

const { argv } = yargs;

main()
  .then(() => console.error("Done"))
  .catch(console.trace);
