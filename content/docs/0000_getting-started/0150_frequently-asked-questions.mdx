export const meta = {
  id: "frequently-asked-questions",
  title: "Frequently Asked Questions",
};

### What is Splitgraph?

Splitgraph is a tool for building, versioning, querying and sharing datasets that works on top of [PostgreSQL](https://www.postgresql.org/) and integrates seamlessly with anything that uses PostgreSQL.

### Is Splitgraph a PostgreSQL extension?

Not quite. The [Splitgraph engine](../architecture/splitgraph-engine) ships as a Docker image and is a customized version of PostgreSQL that is fully compatible with existing clients. In the future, we might repackage Splitgraph as a PostgreSQL extension.

### Can I add Splitgraph to my existing PostgreSQL deployment?

Whilst it is possible to add Splitgraph to existing PostgreSQL deployments, there isn't currently
a simple installation method. If you're interested in doing so, you can follow the instructions
in the [Dockerfile](https://github.com/splitgraph/splitgraph/blob/master/engine/Dockerfile) used
to build the engine or contact us.

You can also add the Splitgraph engine as a PostgreSQL [logical replication client](../ingesting-data/postgres-replication),
which will let you ingest data from existing databases without installing Splitgraph on them.

### Does my data have to be in PostgreSQL to use Splitgraph?

With [foreign data wrappers](../ingesting-data/foreign-data-wrappers/introduction), you can query
data in other databases (including MongoDB, MySQL or PostgreSQL) directly through Spligraph with
[any PostgreSQL client](../integrating-splitgraph/other-clients). You do not need to copy your data into PostgreSQL to use Splitgraph.

### Why PostgreSQL? Why not write your own database?

PostgreSQL is a battle-tested RDBMS with a mature feature set (ACID transactions, authentication,
indexing, crash safety), a long list of commercial and enthusiast users and a rich ecosystem
of extensions and applications. We don't think that features added by Splitgraph warrant building a brand new database.

### Can I use Splitgraph with my existing tools?

Yes. Any PostgreSQL client is able to query Splitgraph datasets (directly or through [layered querying](../large-datasets/layered-querying)),
including [DataGrip](../integrating-splitgraph/datagrip), [pgAdmin](../integrating-splitgraph/pgadmin) or [other clients like pgcli or DBeaver](../integrating-splitgraph/other-clients).

In addition, a lot of existing applications and extensions that work with PostgreSQL can be enhanced
by Splitgraph. We have examples of using Splitgraph with [Jupyter notebooks](../integrating-splitgraph/jupyter-notebooks),
[PostGIS](../integrating-splitgraph/postgis), [PostgREST](../integrating-splitgraph/postgrest),
[dbt](../integrating-splitgraph/dbt) or [Metabase](../integrating-splitgraph/metabase).

Philosophically, we believe that great tools enhance existing abstractions without breaking them:

- Git adds versioning to the filesystem, enhancing it for any tools that use the filesystem. Compilers,
  IDEs or editors don't need to be aware of Git to reap its benefits.
- Docker containerizes applications by letting one use the same commands they would use
  for installing the application on a bare machine. The application itself has no idea it's running
  inside of a container and doesn't need to be rewritten to run in Docker.

We were guided by the same principles when building Splitgraph.

### What's the performance like? Do you have any benchmarks?

We maintain a couple of Jupyter notebooks with benchmarks on [our GitHub](https://github.com/splitgraph/splitgraph/tree/master/examples/benchmarking).

It's difficult to specify what is considered a benchmark for Splitgraph, as for a lot of operations
one would be benchmarking PostgreSQL itself. This is why we haven't run benchmarks like [TPC-DS](http://www.tpc.org/tpcds/) on Splitgraph
(since for maximum performance, it's easy to check out a Splitgraph image into a PostgreSQL schema)
but have tested the overhead of various Splitgraph workloads over PostgreSQL.

In short:

- Committing and checking out Splitgraph images takes slightly less time than writing the same data to PostgreSQL tables (data is moved directly between PostgreSQL tables without query parsing overhead)
- Writing to PostgreSQL tables that are change-tracked by Splitgraph is almost 2x slower
  than writing to untracked tables (Splitgraph uses audit triggers to record changes rather than
  diffing the table at commit time).
- Splitgraph images take up much less (5x-10x) space than equivalent PostgreSQL tables due to it
  using [`cstore_fdw`](https://github.com/citusdata/cstore_fdw) for storage.
- Querying Splitgraph images directly without checkout [(layered querying)](../large-datasets/layered-querying) can sometimes be faster and use less IO
  than querying PostgreSQL tables.

### Can Splitgraph be used for big datasets?

Yes. Splitgraph has a few optimizations that make it suitable for working with large datasets:

- Datasets are partitioned into fragments stored in a [columnar format](../concepts/objects) which is
  superior to row-format storage for OLAP workloads.
- Splitgraph images can be queried without checking them out or even downloading them completely
  [(layered querying)](../large-datasets/layered-querying): Splitgraph can lazily download a small fraction of the table needed for the query. This is still completely seamless to the client application.

Since Splitgraph is built on top of PostgreSQL, same methods that are used for horizontally scaling a
PostgreSQL deployment can also be used on Splitgraph.

### Do I have to register anywhere to use Splitgraph?

No. Splitgraph can be used in a decentralized way, sharing data between two engines like one would
with Git. Here's an [example](https://github.com/splitgraph/splitgraph/tree/master/examples/push-to-other-engine) of getting two Splitgraph instances to synchronize with each other.

It is also possible to push data to S3-compatible storage (like [Minio](https://github.com/splitgraph/splitgraph/tree/master/examples/push-to-object-storage)).

You can use [Splitgraph Cloud](../splitgraph-cloud/introduction) if you wish to
get or share public data or have a [REST API](../splitgraph-cloud/publish-rest-api) generated for your dataset.

### What are the general ideas behind Splitgraph?

Splitgraph tries to provide a framework for dealing with common data science problems.
Rather than taking an existing tool and asking "what would it look like if we applied it to data", we asked ourselves "what do current data management workflows look like, how are they lacking and what can we borrow from other disciplines that can be useful?"

We tried to get our [inspiration](/product/splitgraph/motivation) from multiple tools and best practices in software engineering: versioning, self-contained artifacts, continuous integration, reproducible builds and the idea of tools that improve existing workflows without breaking them and forcing users to change them.

### Why not just use...

#### dbt

dbt is a tool for transforming data inside of the data warehouse that allows users to build up
transformations from reusable and versionable SQL snippets.

dbt is enhanced by Splitgraph: since a Splitgraph engine is also a PostgreSQL instance, dbt can
work against it, getting benefits like version control, packaging and sharing to datasets that it uses and builds.

We have an example of running [dbt](../integrating-splitgraph/dbt) in such a way, swapping between different versions of the
source dataset and looking at their effect on the built dbt model.

Splitgraph also offers its own method of building datasets: [Splitfiles](../concepts/splitfiles). Splitfiles offer Dockerfile-like caching, provenance tracking, fast dataset rebuilds, joins between datasets and full SQL support.

We envision Splitfiles as a replacement for ETL pipelines: instead of a series of processes that transform data between tables in a data warehouse,
transformations are treated as pure functions between isolated self-contained datasets, allowing one to replay any part of their pipeline at any point in time.

#### Databricks

Databricks offers a full take-it-or-leave-it platform for doing data science, including a managed Spark cluster, whereas Splitgraph allows you to pick and choose parts of your workflow that you wish to swap out with Splitgraph.

Rather than ad hoc notebooks, Splitgraph encourages users to transform data using reproducible [Splitfiles](../concepts/splitfiles) that allow hermetic dataset builds.

Same Splitgraph workflows can be applied in development as in production, with the user's local engine. This brings iteration times down and lets you maintain a single codebase for your data transformations.

#### Pachyderm

Pachyderm is used mostly for managing and running distributed data pipelines on flat files (images,
genomics data etc). By specializing in datasets that can be represented as tables in a database,
Splitgraph gets benefits like delta compression on changed data or faster querying speeds.

Similarly to Pachyderm, Splitgraph supports [data lineage (or provenance)](../working-with-data/inspecting-provenance) tracking where the
commands and source datasets that were used to build a particular dataset are recorded in that
dataset's metadata, allowing for them to be replayed or inspected.

Splitgraph can be integrated with Pachyderm using the same methods one would use [for PostgreSQL](https://docs.pachyderm.com/latest/how-tos/splitting-data/splitting/#ingesting-postgressql-data). This can then be used to run a [Splitfile](../concepts/splitfiles) to build a dataset as a
Pachyderm stage.

#### dvc, DataLad, ...

Some tools use [git-annex](https://git-annex.branchable.com/) to version code and data together.
Splitgraph's versioning improves on that by delta compressing changes (which means that bringing
a dataset up to date only requires downloading the changes, rather than the new version) and putting
the data inside of an actual database, making querying it more efficient.

#### FoundationDB, Dolt, ...

A lot of tools that do data versioning also require users to learn to use a brand new database
and rewrite their existing applications to use it. We don't think the benefits from writing a new
database system from scratch outweigh the costs of bringing it up to feature and performance parity with existing
databases and bootstrapping an ecosystem of extensions and applications.
